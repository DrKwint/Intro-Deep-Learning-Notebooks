{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon 7\n",
    "\n",
    "Written by Eleanor Quint\n",
    "\n",
    "Topics:\n",
    "- Techniques for dimension expansion\n",
    "    - Transpose convolutions\n",
    "    - Sub-pixel convolutions\n",
    "    - ProgressiveGAN upscaling\n",
    "- Autoencoding\n",
    "    - Sparse autoencoders\n",
    "    - De-noising autoencoders\n",
    "\n",
    "This is all setup in a IPython notebook so you can run any code you want to experiment with. Feel free to edit any cell, or add some to run your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import matplotlib.pyplot as plt    # to visualize data and draw plots\n",
    "from tqdm import tqdm              # to track progress of loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the code from Hack2 to load MNIST\n",
    "ds = tfds.load('mnist', shuffle_files=True) # this loads a dict with the datasets\n",
    "\n",
    "# We can create an iterator from each dataset\n",
    "# This one iterates through the train data, shuffling and minibatching by 32\n",
    "train_ds = ds['train'].shuffle(1024).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques for dimension expansion\n",
    "\n",
    "Generally, in the classifiers we've used so far, we compress high dimensional representations into lower dimensional ones. Now, we're going to study ways of going from lower dimensional to higher. For this, we're going to define a function `upscale_block` which makes the representation larger, in three different ways.\n",
    "\n",
    "#### Transpose convolutions\n",
    "\n",
    "Although we can upscale unstructured data very easily with a larger dense layer, images are less straightforward to upscale. As the upscaling companion to downscaling convolutions, we can learn to increase the size of images with transpose convolutions. They are sometimes called \"deconvolutions\" because they're the inverse operation of the convolution, but it is actually the transpose (gradient) of a convolution rather than an actual deconvolution. Transpose convolutions are implemented by [tf.keras.layers.Conv2DTranspose\n",
    "](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (32, 28, 28, 1) New shape: (32, 56, 56, 64)\n",
      "Parameters: 640\n"
     ]
    }
   ],
   "source": [
    "def upscale_block(filters, kernel_size=3, scale=2, activation=tf.nn.elu):\n",
    "    \"\"\"conv2d_transpose\"\"\"\n",
    "    return tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides=(scale, scale), padding='same', activation=activation)\n",
    "\n",
    "upscale_op = upscale_block(64)\n",
    "for batch in train_ds:\n",
    "    img = tf.cast(batch['image'], tf.float32)\n",
    "    up_x = upscale_op(img)\n",
    "    print(\"Original shape:\", img.shape, \"New shape:\", up_x.shape)\n",
    "    break\n",
    "\n",
    "num_params = upscale_op.count_params()\n",
    "print('Parameters: ' + str(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-pixel convolutions\n",
    "\n",
    "Another approach is called the sub-pixel convolution, which does a regular convolution with many channels, and then re-orders the data into the height and width dimensions from the channels dimension. For some intuition, a graphic:\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/03a5b2aac53443e6078f0f63b35d4f95d6d54c5d/2-Figure1-1.png\">\n",
    "\n",
    "(Image sourced from [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (32, 28, 28, 1) New shape: (32, 56, 56, 64)\n",
      "Parameters: 2560\n"
     ]
    }
   ],
   "source": [
    "def upscale_block(filters, kernel_size=3, scale=2, activation=tf.nn.elu):\n",
    "    \"\"\"[Sub-Pixel Convolution](https://arxiv.org/abs/1609.05158)\"\"\"\n",
    "    # Increase the number of channels to the number of channels times the scale squared\n",
    "    conv = tf.keras.layers.Conv2D(filters * (scale ** 2), (kernel_size, kernel_size), activation=activation, padding='same')\n",
    "    # Rearrange blocks of (1,1,scale**2) pixels into (scale,scale,1) pixels\n",
    "    rearrange = tf.keras.layers.Lambda(lambda x: tf.nn.depth_to_space(x, scale))\n",
    "    return tf.keras.Sequential([conv, rearrange])\n",
    "    \n",
    "upscale_op = upscale_block(64)\n",
    "for batch in train_ds:\n",
    "    img = tf.cast(batch['image'], tf.float32)\n",
    "    up_x = upscale_op(img)\n",
    "    print(\"Original shape:\", img.shape, \"New shape:\", up_x.shape)\n",
    "    break\n",
    "\n",
    "num_params = upscale_op.count_params()\n",
    "print('Parameters: ' + str(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this upscale method uses more parameters than the transpose convolution. This makes it more powerful and flexible (though this isn't always desirable, e.g., if your model is overfitting).\n",
    "\n",
    "#### ProgressiveGAN upscaling\n",
    "\n",
    "Finally, one technique that's recently found massive success in Nvidia's ProgressiveGAN used to generate high-resolution fake celebrity faces:\n",
    "\n",
    "<img src=\"https://i2.wp.com/robotnyheter.se/wp-content/uploads/2018/01/Nvidia_GAN_ansikten.jpg?w=1561\" width=\"70%\">\n",
    "\n",
    "None of these are real photos, they've all been upsampled in the GAN framework from Gaussian noise with an architecture that uses this technique. It upscales using a classical algorithm like bilinear upscaling and then transforms the output with convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (32, 28, 28, 1) New shape: (32, 56, 56, 64)\n",
      "Parameters: 640\n"
     ]
    }
   ],
   "source": [
    "def upscale_block(filters, kernel_size=3, scale=2, activation=tf.nn.elu):\n",
    "    \"\"\" similar to the upsampling used in [ProgressiveGAN](https://arxiv.org/pdf/1710.10196.pdf) \"\"\"\n",
    "    def upscale(x):\n",
    "        n, w, h, c = x.get_shape().as_list()\n",
    "        return tf.image.resize(x, [scale*h, scale*w])\n",
    "    \n",
    "    upscale_layer = tf.keras.layers.Lambda(upscale)\n",
    "    conv = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', activation=activation)\n",
    "    return tf.keras.Sequential([upscale_layer, conv])\n",
    "\n",
    "upscale_op = upscale_block(64)\n",
    "for batch in train_ds:\n",
    "    img = tf.cast(batch['image'], tf.float32)\n",
    "    up_x = upscale_op(img)\n",
    "    print(\"Original shape:\", img.shape, \"New shape:\", up_x.shape)\n",
    "    break\n",
    "\n",
    "num_params = upscale_op.count_params()\n",
    "print('Parameters: ' + str(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoding\n",
    "\n",
    "Generally, autoencoding is learning \"a complicated identity function\". This makes it a form of unsupervised learning, which doesn't require data to be explicitly labeled, but instead looks for patterns and trends in data. Typically the complication is to bottleneck the size of the representation, but can also be more varied. We'll look at code for sparse autoencoders and de-noising autoencoders.\n",
    "\n",
    "First we'll define some preliminaries that we'll use in both architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_block(filters, kernel_size=3, scale=2, activation=tf.nn.elu):\n",
    "    \"\"\"[Sub-Pixel Convolution](https://arxiv.org/abs/1609.05158)\"\"\"\n",
    "    # Increase the number of channels to the number of channels times the scale squared\n",
    "    conv = tf.keras.layers.Conv2D(filters * (scale**2),\n",
    "                                  (kernel_size, kernel_size),\n",
    "                                  activation=activation,\n",
    "                                  padding='same')\n",
    "    # Rearrange blocks of (1,1,scale**2) pixels into (scale,scale,1) pixels\n",
    "    rearrange = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.nn.depth_to_space(x, scale))\n",
    "    return tf.keras.Sequential([conv, rearrange])\n",
    "\n",
    "\n",
    "class UpscaleBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, number, kernel_size=3, activation=tf.nn.swish):\n",
    "        super().__init__(name=\"UpscaleBlock\" + str(number))\n",
    "        self.activation = activation\n",
    "        self.kernel_size = kernel_size\n",
    "        self.is_built = False\n",
    "\n",
    "    def build(self, x):\n",
    "        channels = x.shape.as_list()[-1]\n",
    "        filters = channels // 2\n",
    "\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        conv1 = upscale_block(filters)\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        conv2 = tf.keras.layers.Conv2D(filters,\n",
    "                                       self.kernel_size,\n",
    "                                       padding='same')\n",
    "        self.main_network = [self.activation, bn1, conv1, self.activation, bn2, conv2]\n",
    "\n",
    "        self.skip_connection = upscale_block(filters)\n",
    "        self.se_activate = SqueezeExcite(filters)\n",
    "        self.is_built = True\n",
    "\n",
    "    def __call__(self, input_):\n",
    "        if not self.is_built:\n",
    "            self.build(input_)\n",
    "        x = input_\n",
    "        for layer in self.main_network:\n",
    "            x = layer(x)\n",
    "        output = x\n",
    "        skip = self.skip_connection(input_)\n",
    "        return skip + 0.1 * output\n",
    "\n",
    "\n",
    "class FactorizedReduce(tf.Module):\n",
    "    \"\"\"Downscale version of the sub-pixel convolution which re-arranges pixels\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(FactorizedReduce, self).__init__()\n",
    "        assert channels % 2 == 0\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(channels // 4, 1, strides=2)\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(channels // 4, 1, strides=2)\n",
    "        self.conv_3 = tf.keras.layers.Conv2D(channels // 4, 1, strides=2)\n",
    "        self.conv_4 = tf.keras.layers.Conv2D(channels - 3 * (channels // 4),\n",
    "                                             1,\n",
    "                                             strides=2)\n",
    "        self.convs = [self.conv_1, self.conv_2, self.conv_3, self.conv_4]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Assumes NHCW data\"\"\"\n",
    "        assert x.shape[2] > 1\n",
    "        assert x.shape[3] > 1\n",
    "        out = tf.nn.swish(x)\n",
    "        conv1 = self.conv_1(out)\n",
    "        conv2 = self.conv_2(out[:, :, 1:, 1:])\n",
    "        conv3 = self.conv_3(out[:, :, :, 1:])\n",
    "        conv4 = self.conv_4(out[:, :, 1:, :])\n",
    "        out = tf.concat([conv1, conv2, conv3, conv4], -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SqueezeExcite(tf.Module):\n",
    "    \"\"\"Activation function that performs gating\"\"\"\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "        num_hidden = max(out_channels // 16, 4)\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(num_hidden), tf.keras.layers.Lambda(tf.nn.relu),\n",
    "            tf.keras.layers.Dense(out_channels), tf.keras.layers.Lambda(tf.nn.sigmoid)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"The choice of axes assumes we're working with NHWC data\"\"\"\n",
    "        ax = tf.math.reduce_mean(x, axis=[1, 2])\n",
    "        # data should be flat at this po,int\n",
    "        bx = self.net(ax)\n",
    "        cx = tf.expand_dims(tf.expand_dims(bx, 1), 1)\n",
    "        return cx * x\n",
    "\n",
    "\n",
    "class DownscaleBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, number, kernel_size=3, activation=tf.nn.swish):\n",
    "        super().__init__(name=\"DownscaleBlock\" + str(number))\n",
    "        self.activation = activation\n",
    "        self.kernel_size = kernel_size\n",
    "        self.is_built = False\n",
    "\n",
    "    def build(self, x):\n",
    "        channels = x.shape.as_list()[-1]\n",
    "        filters = channels * 2\n",
    "\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        conv1 = tf.keras.layers.Conv2D(filters,\n",
    "                                       self.kernel_size,\n",
    "                                       strides=2,\n",
    "                                       padding='same')\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        conv2 = tf.keras.layers.Conv2D(filters,\n",
    "                                       self.kernel_size,\n",
    "                                       padding='same')\n",
    "        self.main_network = [self.activation, bn1, conv1, self.activation, bn2, conv2]\n",
    "\n",
    "        self.skip_connection = FactorizedReduce(filters)\n",
    "        self.se_activate = SqueezeExcite(filters)\n",
    "        self.is_built = True\n",
    "\n",
    "    def __call__(self, input_):\n",
    "        if not self.is_built:\n",
    "            self.build(input_)\n",
    "        x = input_\n",
    "        for layer in self.main_network:\n",
    "            x = layer(x)\n",
    "        output = x\n",
    "        skip = self.skip_connection(input_)\n",
    "        return skip + 0.1 * output\n",
    "\n",
    "\n",
    "encoder_network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same',\n",
    "                           activation=tf.nn.swish),  #28,28,16\n",
    "    DownscaleBlock(1),  # 14,14,32\n",
    "    DownscaleBlock(2),  # 7,7,64\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same',\n",
    "                           activation=tf.nn.swish),  # 7,7,64\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same',\n",
    "                           activation=tf.nn.swish),  # 7,7,16\n",
    "    tf.keras.layers.Conv2D(1, 3, padding='same'),  # 7,7,4\n",
    "])\n",
    "\n",
    "decoder_network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(4, 3, padding='same',\n",
    "                           activation=tf.nn.swish),  # 7,7,4\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same',\n",
    "                           activation=tf.nn.swish),  # 7,7,16\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same',\n",
    "                           activation=tf.nn.swish),  # 7,7,64\n",
    "    UpscaleBlock(1),  # 14,14,32\n",
    "    UpscaleBlock(2),  # 28,28,16\n",
    "    tf.keras.layers.Conv2D(4, 3, padding='same',\n",
    "                           activation=tf.nn.swish),  #28,28,4\n",
    "    tf.keras.layers.Conv2D(1, 3, padding='same'),  #28,28,16\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1) (32, 7, 7, 1) (32, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds:\n",
    "    x = tf.cast(batch['image'], tf.float32)\n",
    "    code = encoder_network(x)\n",
    "    output = decoder_network(code)\n",
    "    break\n",
    "print(x.shape, code.shape, output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse autoenoding\n",
    "\n",
    "Although we bottleneck the representation in normal autoencoding by reducing the dimensionality, sparse autoencoders can actually increase it, but restrict it to be sparsely activated with L1 regularization using [tf.norm](https://www.tensorflow.org/api_docs/python/tf/norm) or KL-divergence. This has the effect of only having non-zero values in a few dimensions, effectively bottlenecking each representation, but giving a greater variety of dimensions to choose to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 250/1875 [01:03<06:55,  3.91it/s]\n"
     ]
    }
   ],
   "source": [
    "def sparse_autoencoder_loss(x, code, x_hat, sparsity_coeff=5.):\n",
    "    sparsity_loss = tf.norm(code, ord=1, axis=1)\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(x_hat - x)) # Mean Square Error\n",
    "    total_loss = reconstruction_loss + sparsity_coeff * sparsity_loss\n",
    "    return total_loss\n",
    "\n",
    "max_steps = 250\n",
    "step = 0\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "for batch in tqdm(train_ds):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x = tf.cast(batch['image'], tf.float32)\n",
    "        code = encoder_network(x)\n",
    "        output = decoder_network(code)\n",
    "        loss = sparse_autoencoder_loss(x, code, output)\n",
    "    gradient = tape.gradient(loss, encoder_network.trainable_variables + decoder_network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, encoder_network.trainable_variables + decoder_network.trainable_variables))\n",
    "    step += 1\n",
    "    if step > max_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1.4908686e-01]\n",
      "  [ 4.2339283e-01]\n",
      "  [ 9.4308460e-01]\n",
      "  [-2.1568465e+00]\n",
      "  [-1.8790067e+00]\n",
      "  [-3.3020542e+00]\n",
      "  [ 7.0723665e-01]]\n",
      "\n",
      " [[-1.0250514e+00]\n",
      "  [-1.4052591e+00]\n",
      "  [ 3.2976835e+00]\n",
      "  [ 4.0087171e+00]\n",
      "  [ 5.8101668e+00]\n",
      "  [ 7.7373619e+00]\n",
      "  [ 3.4203649e+00]]\n",
      "\n",
      " [[-8.2330769e-01]\n",
      "  [ 1.8156372e+00]\n",
      "  [ 1.9981045e+00]\n",
      "  [-6.8335767e+00]\n",
      "  [-9.0412722e+00]\n",
      "  [-1.3096324e+01]\n",
      "  [-1.0754415e+01]]\n",
      "\n",
      " [[ 2.9980594e-01]\n",
      "  [-3.8292920e-03]\n",
      "  [-1.4927105e+01]\n",
      "  [-8.2689447e+00]\n",
      "  [-2.1763744e+00]\n",
      "  [-4.2708516e+00]\n",
      "  [-9.8600435e+00]]\n",
      "\n",
      " [[-1.2850415e+00]\n",
      "  [-5.4875135e+00]\n",
      "  [ 2.7838287e+00]\n",
      "  [-9.3007135e+00]\n",
      "  [-5.9968286e+00]\n",
      "  [ 5.6513939e+00]\n",
      "  [ 6.7475283e-01]]\n",
      "\n",
      " [[ 1.0874690e+00]\n",
      "  [-7.9813399e+00]\n",
      "  [-8.1224051e+00]\n",
      "  [-1.0932647e+01]\n",
      "  [-2.8595920e+00]\n",
      "  [ 6.9433439e-01]\n",
      "  [-9.2574632e-01]]\n",
      "\n",
      " [[-2.0320784e-01]\n",
      "  [ 1.8417556e+00]\n",
      "  [-6.8605304e-01]\n",
      "  [-2.8115833e-01]\n",
      "  [ 3.6600363e-01]\n",
      "  [-1.0034875e+00]\n",
      "  [ 6.3364446e-01]]], shape=(7, 7, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Show a code (which should be pretty sparse)\n",
    "print(code[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original and Reconstruction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x151831130>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show reconstruction\n",
    "# This will perform much better with more training and hyperparameter tuning\n",
    "print(\"Original and Reconstruction\")\n",
    "side_by_side = tf.concat([x[0], output[0]], 1).numpy()\n",
    "plt.imshow(side_by_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising Autoencoder\n",
    "\n",
    "Another way to force the an autoencoder to learn the features of data is to force it to map noisy, corrupted versions of the data back to the original. This is usually accomplished by manually adding noice (e.g., Gaussian), but may also be useful in real world settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 250/1875 [01:09<07:29,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "NOISE_COEFF = 10.\n",
    "\n",
    "def autoencoder_loss(x, x_hat):\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(x_hat - x)) # Mean Square Error\n",
    "    total_loss = reconstruction_loss\n",
    "    return total_loss\n",
    "\n",
    "max_steps = 250\n",
    "step = 0\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "for batch in tqdm(train_ds):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x = tf.cast(batch['image'], tf.float32)\n",
    "        code = encoder_network(x + tf.random.normal(x.shape))\n",
    "        output = decoder_network(code)\n",
    "        loss = autoencoder_loss(x, output)\n",
    "    gradient = tape.gradient(loss, encoder_network.trainable_variables + decoder_network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, encoder_network.trainable_variables + decoder_network.trainable_variables))\n",
    "    step += 1\n",
    "    if step > max_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[  2.211234 ]\n",
      "  [  7.521559 ]\n",
      "  [  8.363447 ]\n",
      "  [  3.909775 ]\n",
      "  [ -4.176254 ]\n",
      "  [ -7.194827 ]\n",
      "  [  0.7633354]]\n",
      "\n",
      " [[ -4.698588 ]\n",
      "  [ -4.585447 ]\n",
      "  [-13.215098 ]\n",
      "  [-24.94045  ]\n",
      "  [-18.87241  ]\n",
      "  [ -6.911929 ]\n",
      "  [ 11.298568 ]]\n",
      "\n",
      " [[  3.557348 ]\n",
      "  [ -4.358646 ]\n",
      "  [-16.024612 ]\n",
      "  [ -2.2225645]\n",
      "  [-27.036407 ]\n",
      "  [-32.0292   ]\n",
      "  [  2.2061718]]\n",
      "\n",
      " [[ -5.4242783]\n",
      "  [  1.7809695]\n",
      "  [-17.733469 ]\n",
      "  [-37.680603 ]\n",
      "  [-15.982113 ]\n",
      "  [ -2.2150962]\n",
      "  [  2.2674172]]\n",
      "\n",
      " [[  2.9063895]\n",
      "  [  2.2730253]\n",
      "  [ -1.3933939]\n",
      "  [-15.200351 ]\n",
      "  [-42.154182 ]\n",
      "  [-14.424747 ]\n",
      "  [  9.177154 ]]\n",
      "\n",
      " [[ -2.0921323]\n",
      "  [-20.077364 ]\n",
      "  [-15.209964 ]\n",
      "  [-13.694405 ]\n",
      "  [-27.923115 ]\n",
      "  [-14.100279 ]\n",
      "  [  3.0598977]]\n",
      "\n",
      " [[  2.0658998]\n",
      "  [-13.007997 ]\n",
      "  [-32.147427 ]\n",
      "  [-28.386297 ]\n",
      "  [  1.5882217]\n",
      "  [  4.4024343]\n",
      "  [ -6.3736825]]], shape=(7, 7, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Show a code (which should be pretty sparse)\n",
    "print(code[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original and Reconstruction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15186d190>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show reconstruction\n",
    "# This will perform much better with more training and hyperparameter tuning\n",
    "print(\"Original and Reconstruction\")\n",
    "side_by_side = tf.concat([x[0] + NOISE_COEFF * tf.random.normal(x[0].shape), output[0]], 1).numpy()\n",
    "plt.imshow(side_by_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Try out the autoencoder architecture above and test out the relationship between the mean squared error and the size of the latent variable (e.g., the (7,7,1) shape used above) is using the above architecture on MNIST after some training. Try 2 or 3 code sizes (or more if you like) and report the parameters of a best fit line \n",
    "\n",
    "### Coda\n",
    "\n",
    "#### [Progressive GAN latent space interpolation on Youtube](https://youtu.be/XOxxPcy5Gr4?t=1m48s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_notebooks",
   "language": "python",
   "name": "dl_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
